# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Q6MoefJKm3h00fDawVLhj_z3q0aOQXn
"""

import pandas as pd

# Load datasets
df_credit = pd.read_csv("credit_card_default.csv")
df_trans = pd.read_csv("bank_customer_transactions.csv")
df_notes = pd.read_csv("loan_notes.csv")

# Merge datasets on user_id
df_merged = df_credit.merge(df_trans, on='user_id', how='inner')
df_merged = df_merged.merge(df_notes, on='user_id', how='left')

# Drop duplicate target column if needed
if 'Default' in df_merged.columns and 'default' in df_merged.columns:
    df_merged = df_merged.drop(columns=['Default'])  # Keep only one target column

# Show the merged dataset
print(df_merged.head())

# Check for missing values
print(df_merged.isnull().sum())

# Fill or drop missing values (simple approach for now)
df_merged = df_merged.dropna()

# Encode 'Education' if it's categorical
if df_merged['education'].dtype == 'object':
    df_merged['education'] = df_merged['education'].astype('category').cat.codes

from sklearn.feature_extraction.text import TfidfVectorizer

# Fill missing loan_notes with empty string
df_merged['note_text'] = df_merged['note_text'].fillna("")

# Initialize TF-IDF
tfidf = TfidfVectorizer(max_features=100)  # limit to top 100 features
tfidf_matrix = tfidf.fit_transform(df_merged['note_text'])

# Convert to DataFrame
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())

# Reset index to merge with original df
df_tfidf.reset_index(drop=True, inplace=True)
df_merged.reset_index(drop=True, inplace=True)

# Combine structured + text features
df_final = pd.concat([df_merged.drop(columns=['note_text']), df_tfidf], axis=1)

non_numeric = X.select_dtypes(include='object').columns
print("‚ùó Non-numeric columns:", non_numeric.tolist())

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix

# Step 1: Drop unnecessary columns
df_cleaned = df_final.drop(columns=['user_id', 'date'], errors='ignore')

# Step 2: Separate features and target
X = df_cleaned.drop(columns=['default'])
y = df_cleaned['default']

# ‚úÖ Step 3: Label encode all non-numeric columns without checking dtype
label_encoders = {}
for col in X.columns:
    try:
        X[col] = pd.to_numeric(X[col])  # try converting to numeric
    except:
        le = LabelEncoder()
        X[col] = le.fit_transform(X['category'].astype(str))
        label_encoders[col] = le

# Step 4: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Train the model
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# Step 6: Evaluate
y_pred = clf.predict(X_test)
print("üìä Classification Report:\n", classification_report(y_test, y_pred))
print("üßæ Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

pip install shap

pip install shap

import shap

# Initialize explainer
explainer = shap.TreeExplainer(clf)

# Get SHAP values for test data
shap_values = explainer.shap_values(X_test)

import numpy as np

# Get class 1 SHAP values
shap_vals_class1 = shap_values[1]  # class index for 'default = 1'

# Loop through few test samples
for i in range(5):  # Show for first 5 customers
    print(f"\nüßë Customer {i+1}")
    sample = X_test.iloc[i]
    shap_row = shap_vals_class1[i]

    # Get top 3 features by absolute SHAP value
    top3_idx = np.argsort(np.abs(shap_row))[-3:][::-1]
    for idx in top3_idx:
        feature = X_test.columns[idx]
        value = sample[feature]
        shap_val = shap_row[idx]
        print(f"üîπ {feature} = {value} (Impact: {shap_val:.4f})")

import numpy as np
import pandas as pd

# Get SHAP values for class 1 (default = 1)
shap_vals_class1 = shap_values[1]

# DataFrame to store top 3 features for each test customer
explanations = []

for i in range(len(X_test)):
    sample = X_test.iloc[4]
    shap_row = shap_vals_class1[4]

    # Get top 3 features by absolute SHAP value
    top3_idx = np.argsort(np.abs(shap_row))[-5:][::-1]

    top_features = []
    for idx in top3_idx:
        feature = X_test.columns[idx]
        value = sample[feature]
        shap_val = shap_row[idx]
        top_features.append(f"{feature} = {value} (impact: {shap_val:.3f})")

    explanations.append({
        "Customer_Index": i,
        "Predicted_Default": clf.predict([sample])[0],
        "Top_Feature_1": top_features[0],
        "Top_Feature_2": top_features[1],
    })

# Convert to DataFrame
explanation_df = pd.DataFrame(explanations)
explanation_df.head()

explanation_df.to_csv("customer_risk_explanations.csv", index=False)

import streamlit as st
import pandas as pd

# Load SHAP results
df = pd.read_csv("customer_risk_explanations.csv")

st.title("üîç Credit Risk Explanation Dashboard")

st.sidebar.header("Customer Selector")
customer_id = st.sidebar.selectbox("Choose customer index", df["Customer_Index"])

row = df[df["Customer_Index"] == customer_id].iloc[0]

st.markdown(f"## üßë Customer {customer_id}")
st.write(f"**Predicted Default:** {'Yes' if row['Predicted_Default'] == 1 else 'No'}")
st.write("### üí¨ Top Contributing Features:")
st.write(f"1Ô∏è‚É£ {row['Top_Feature_1']}")
st.write(f"2Ô∏è‚É£ {row['Top_Feature_2']}")

